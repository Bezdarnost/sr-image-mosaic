{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded77498-4ac6-4a2b-867f-ee2fccc3a791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aman/anaconda3/envs/contrast/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import timm\n",
    "import faiss\n",
    "import torch\n",
    "from hat_arch import HAT\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64f01a89-4b61-4a39-bdb9-560307703047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|████████████████████| 3450/3450 [02:34<00:00, 22.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to create the patches directory\n",
    "def create_patches_dir(output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "# Function to split the image into patches\n",
    "def split_image_into_patches(image, patch_size, overlap):\n",
    "    h, w, _ = image.shape\n",
    "    stride = int(patch_size * (1 - overlap))\n",
    "    \n",
    "    patches = []\n",
    "    for y in range(0, h - patch_size + 1, stride):\n",
    "        for x in range(0, w - patch_size + 1, stride):\n",
    "            patch = image[y:y + patch_size, x:x + patch_size]\n",
    "            patches.append(patch)\n",
    "    return patches\n",
    "\n",
    "# Function to process a single image and save patches\n",
    "def process_image(img_info):\n",
    "    img_path, output_folder, patch_size, overlap = img_info\n",
    "    image = cv2.imread(img_path)\n",
    "    \n",
    "    # Resize the image to 1/4th of its original size\n",
    "    image = cv2.resize(image, (image.shape[1] // 32, image.shape[0] // 32), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    patches = split_image_into_patches(image, patch_size, overlap)\n",
    "    img_file = os.path.basename(img_path)\n",
    "\n",
    "    for i, patch in enumerate(patches):\n",
    "        patch_name = f\"{os.path.splitext(img_file)[0]}_patch_{i}.png\"\n",
    "        patch_path = os.path.join(output_folder, patch_name)\n",
    "        cv2.imwrite(patch_path, patch)\n",
    "\n",
    "# Main function to process the folders using multiprocessing\n",
    "def process_folders(input_folder1, input_folder2, output_folder, patch_size=8, overlap=0.5, num_workers=8):\n",
    "    create_patches_dir(output_folder)\n",
    "    \n",
    "    image_info = []\n",
    "    \n",
    "    # Gather image paths from both folders\n",
    "    for folder in [input_folder1, input_folder2]:\n",
    "        image_files = [f for f in os.listdir(folder) if f.endswith(('.png', '.jpg', '.jpeg', '.tif'))]\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(folder, img_file)\n",
    "            image_info.append((img_path, output_folder, patch_size, overlap))\n",
    "    \n",
    "    # Use multiprocessing to process the images\n",
    "    with Pool(num_workers) as pool:\n",
    "        list(tqdm(pool.imap_unordered(process_image, image_info), total=len(image_info), desc=\"Processing images\"))\n",
    "\n",
    "# Input folders (replace with actual folder paths)\n",
    "input_folder1 = \"Flickr2K\"\n",
    "input_folder2 = \"DIV2K\"\n",
    "output_folder = \"patches\"\n",
    "\n",
    "# Call the function\n",
    "process_folders(input_folder1, input_folder2, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2051dcb4-8adf-4a41-b5ad-3a2c1f42a972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aman/anaconda3/envs/contrast/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aman/anaconda3/envs/contrast/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Processing images: 100%|████████████████████| 6812/6812 [10:15<00:00, 11.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset to load images\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_files = [f for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg', '.tif'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_folder, self.image_files[idx])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.image_files[idx]\n",
    "\n",
    "# Initialize the VGG model from torchvision\n",
    "def get_vgg_model():\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-2])  # Remove the classifier layers, keep convolutional layers\n",
    "    model = model[0][:-9]\n",
    "    model = model.cuda()  # Move model to GPU\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Function to process a batch of images and extract embeddings\n",
    "def extract_embeddings_batch(model, batch_images):\n",
    "    # Move the batch of images to the GPU\n",
    "    batch_tensor = batch_images.cuda()  # No need to stack, DataLoader already returns tensors\n",
    "    \n",
    "    # Extract embeddings (forward pass)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(batch_tensor).flatten(1).cpu().numpy().astype(np.float32)  # Flatten and convert to numpy\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Main function to process images in batches and save embeddings to CSV\n",
    "def process_images_and_save_embeddings(image_folder, output_csv, batch_size=64, num_workers=8):\n",
    "    # Get the VGG model\n",
    "    model = get_vgg_model()\n",
    "\n",
    "    # Use transforms suitable for VGG16 models\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((8, 8)),  # Resize to 8\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Create a custom dataset\n",
    "    dataset = ImageDataset(image_folder, transform)\n",
    "\n",
    "    # Use DataLoader with multiple workers to parallelize data loading\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # Initialize the CSV writer and append after the first batch\n",
    "    with open(output_csv, 'a') as f_output:\n",
    "        for batch_images, image_files in tqdm(dataloader, desc=\"Processing images\"):\n",
    "            # Extract embeddings for the batch\n",
    "            embeddings = extract_embeddings_batch(model, batch_images)\n",
    "\n",
    "            # Write embeddings to the CSV file\n",
    "            for img_file, embedding in zip(image_files, embeddings):\n",
    "                f_output.write(f\"{img_file},\" + \",\".join(map(str, embedding)) + \"\\n\")\n",
    "                \n",
    "            # Clear memory after each batch\n",
    "            del batch_images, image_files, embeddings  # Delete large variables\n",
    "            torch.cuda.empty_cache()  # Clear unused VRAM\n",
    "\n",
    "    # Final memory cleanup\n",
    "    del model  # Remove model from memory\n",
    "    torch.cuda.empty_cache()  # Clear any remaining VRAM\n",
    "\n",
    "# Define input and output paths\n",
    "image_folder = \"patches\"\n",
    "output_csv = \"perceptual_embeddings.csv\"\n",
    "\n",
    "# Call the function to process images and save embeddings\n",
    "process_images_and_save_embeddings(image_folder, output_csv, batch_size=64, num_workers=8)\n",
    "\n",
    "# Complete cleanup after processing\n",
    "torch.cuda.empty_cache()  # Ensure all VRAM is cleared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "572a8050-ff0f-48fe-baf0-3b0e416e728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aman/anaconda3/envs/contrast/lib/python3.11/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/tmp/ipykernel_10131/611807520.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('Real_HAT_GAN_sharper.pth')\n",
      "Processing tiles: 100%|███████████████████████████| 2/2 [00:14<00:00,  7.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the HAT model\n",
    "def get_hat_model():\n",
    "    model = HAT(\n",
    "        upscale=4,\n",
    "        in_chans=3,\n",
    "        img_size=64,\n",
    "        window_size=16,\n",
    "        compress_ratio=3,\n",
    "        squeeze_factor=30,\n",
    "        conv_scale=0.01,\n",
    "        overlap_ratio=0.5,\n",
    "        img_range=1.,\n",
    "        depths=[6]*6,\n",
    "        embed_dim=180,\n",
    "        num_heads=[6]*6,\n",
    "        mlp_ratio=2,\n",
    "        upsampler='pixelshuffle',\n",
    "        resi_connection='1conv'\n",
    "    )\n",
    "    # Load weights\n",
    "    checkpoint = torch.load('Real_HAT_GAN_sharper.pth')\n",
    "    model.load_state_dict(checkpoint['params_ema'], strict=True)\n",
    "    model.eval()\n",
    "    model = model.cuda()\n",
    "    return model\n",
    "\n",
    "# Function to perform tiled inference\n",
    "def super_resolve_image(model, input_image_path, tile_size=256, tile_overlap=32, window_size=16):\n",
    "    # Open the input image\n",
    "    img = Image.open(input_image_path).convert('RGB')\n",
    "    img_width, img_height = img.size\n",
    "\n",
    "    # Prepare the image transform\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    img_range = model.img_range if hasattr(model, 'img_range') else 1.0\n",
    "    upscale = model.upscale if hasattr(model, 'upscale') else 1\n",
    "\n",
    "    # Convert image to tensor\n",
    "    img_tensor = img_transform(img).unsqueeze(0).cuda()  # Add batch dimension\n",
    "\n",
    "    # Pad image so that dimensions are multiples of window_size\n",
    "    mod_pad_w = (window_size - img_width % window_size) % window_size\n",
    "    mod_pad_h = (window_size - img_height % window_size) % window_size\n",
    "    padding = (0, 0, mod_pad_w, mod_pad_h)  # Pad right and bottom\n",
    "    img_padded = F.pad(img_tensor, padding, mode='reflect')\n",
    "\n",
    "    _, _, padded_height, padded_width = img_padded.shape\n",
    "\n",
    "    # Prepare output tensor\n",
    "    output_height = padded_height * upscale\n",
    "    output_width = padded_width * upscale\n",
    "    output = torch.zeros(1, 3, output_height, output_width).cuda()\n",
    "\n",
    "    # Calculate the number of tiles\n",
    "    stride = tile_size - tile_overlap\n",
    "    tiles_x = (padded_width + stride - 1) // stride\n",
    "    tiles_y = (padded_height + stride - 1) // stride\n",
    "\n",
    "    # Loop over tiles\n",
    "    for y in tqdm(range(tiles_y), desc='Processing tiles'):\n",
    "        for x in range(tiles_x):\n",
    "            start_x = x * stride\n",
    "            start_y = y * stride\n",
    "            end_x = min(start_x + tile_size, padded_width)\n",
    "            end_y = min(start_y + tile_size, padded_height)\n",
    "\n",
    "            # Extract tile\n",
    "            input_tile = img_padded[:, :, start_y:end_y, start_x:end_x]\n",
    "\n",
    "            # Calculate the required padding for the tile to be divisible by window_size\n",
    "            tile_height = end_y - start_y\n",
    "            tile_width = end_x - start_x\n",
    "            pad_h = (window_size - tile_height % window_size) % window_size\n",
    "            pad_w = (window_size - tile_width % window_size) % window_size\n",
    "\n",
    "            # Pad tile if necessary\n",
    "            if pad_h > 0 or pad_w > 0:\n",
    "                input_tile = F.pad(input_tile, (0, pad_w, 0, pad_h), mode='reflect')\n",
    "\n",
    "            # Super-resolve the tile\n",
    "            with torch.no_grad():\n",
    "                output_tile = model(input_tile)\n",
    "\n",
    "            # Remove padding from output_tile if input_tile was padded\n",
    "            if pad_h > 0 or pad_w > 0:\n",
    "                output_tile = output_tile[:, :, :tile_height * upscale, :tile_width * upscale]\n",
    "\n",
    "            # Determine placement in output tensor\n",
    "            out_start_x = start_x * upscale\n",
    "            out_start_y = start_y * upscale\n",
    "            out_end_x = out_start_x + output_tile.shape[-1]\n",
    "            out_end_y = out_start_y + output_tile.shape[-2]\n",
    "\n",
    "            # Place the tile into the output image\n",
    "            output[:, :, out_start_y:out_end_y, out_start_x:out_end_x] = output_tile\n",
    "\n",
    "            # Clear cache to save VRAM\n",
    "            del input_tile, output_tile\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Crop to original image size multiplied by upscale factor\n",
    "    output = output[:, :, :img_height * upscale, :img_width * upscale]\n",
    "\n",
    "    # Convert to image and save\n",
    "    output_img = output.squeeze(0).cpu().clamp(0, img_range)\n",
    "    if img_range != 1.0:\n",
    "        output_img = output_img / img_range\n",
    "    output_img = transforms.ToPILImage()(output_img)\n",
    "\n",
    "    # Save the image with 'sr' appended to the filename\n",
    "    base_name, ext = os.path.splitext(os.path.basename(input_image_path))\n",
    "    output_image_path = os.path.join(\n",
    "        os.path.dirname(input_image_path), f\"{base_name}_sr{ext}\"\n",
    "    )\n",
    "    output_img.save(output_image_path)\n",
    "\n",
    "    # Clean up\n",
    "    del output, img_tensor, img_padded\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Main code\n",
    "if __name__ == '__main__':\n",
    "    # Initialize the model\n",
    "    model = get_hat_model()\n",
    "\n",
    "    # Input image path (replace with your image path)\n",
    "    input_image_path = 'photo_2024-09-30_16-47-00.jpg'  # Replace with your image\n",
    "\n",
    "    # Perform super-resolution\n",
    "    super_resolve_image(model, input_image_path, tile_size=512, tile_overlap=32, window_size=16)\n",
    "\n",
    "    # Clean up\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980812b9-160a-4c11-abcd-dbae1116dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for SR image patches...\n",
      "Loading precomputed embeddings...\n",
      "Finding nearest patches...\n",
      "Creating the mosaic image...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 307200/307200 [00:24<00:00, 12326.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosaic image saved to mosaic_image.jpg\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(embedding_csv):\n",
    "    # Load embeddings and filenames\n",
    "    df = pd.read_csv(embedding_csv, header=None)\n",
    "    filenames = df[0].values\n",
    "    embeddings = df.drop(0, axis=1).values.astype('float32')\n",
    "    return embeddings, filenames\n",
    "\n",
    "def initialize_vgg_model():\n",
    "    # Initialize the VGG16 model\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-2])  # Remove the classifier layers, keep convolutional layers\n",
    "    model = model[0][:-9]\n",
    "    model = model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def compute_patch_embeddings(model, patches, transform):\n",
    "    # Compute embeddings for a list of image patches\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for patch in patches:\n",
    "            # Apply transformations\n",
    "            input_tensor = transform(patch).unsqueeze(0).cuda()\n",
    "            # Extract embedding\n",
    "            embedding = model(input_tensor).flatten(1).cpu().numpy().astype(np.float32)\n",
    "            embeddings.append(embedding)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def find_nearest_patches(patch_embeddings, database_embeddings):\n",
    "    # Use FAISS for efficient nearest neighbor search\n",
    "    index = faiss.IndexFlatL2(database_embeddings.shape[1])\n",
    "    index.add(database_embeddings)\n",
    "    distances, indices = index.search(patch_embeddings, 1)  # k=1 for nearest neighbor\n",
    "    return indices.flatten()\n",
    "\n",
    "def create_mosaic_image(sr_image_path, patches_folder, embeddings_csv, output_image_path):\n",
    "    # Load the SR image\n",
    "    sr_image = Image.open(sr_image_path).convert('RGB')\n",
    "    sr_width, sr_height = sr_image.size\n",
    "\n",
    "    # Crop the SR image to make dimensions divisible by 16\n",
    "    new_width = (sr_width // 8) * 8\n",
    "    new_height = (sr_height // 8) * 8\n",
    "    sr_image = sr_image.crop((0, 0, new_width, new_height))\n",
    "\n",
    "    # Divide the SR image into 16 patches\n",
    "    patches = []\n",
    "    positions = []  # To keep track of where each patch belongs\n",
    "    for y in range(0, new_height, 8):\n",
    "        for x in range(0, new_width, 8):\n",
    "            patch = sr_image.crop((x, y, x + 8, y + 8))\n",
    "            patches.append(patch)\n",
    "            positions.append((x, y))\n",
    "\n",
    "    # Initialize the VGG model and transform\n",
    "    vgg_model = initialize_vgg_model()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((8, 8)),  # Ensure size is 8\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for the SR patches\n",
    "    print(\"Computing embeddings for SR image patches...\")\n",
    "    sr_patch_embeddings = compute_patch_embeddings(vgg_model, patches, transform)\n",
    "\n",
    "    # Load the precomputed embeddings from the CSV file\n",
    "    print(\"Loading precomputed embeddings...\")\n",
    "    database_embeddings, database_filenames = load_embeddings(embeddings_csv)\n",
    "\n",
    "    # Find the nearest patches from the dataset\n",
    "    print(\"Finding nearest patches...\")\n",
    "    nearest_indices = find_nearest_patches(sr_patch_embeddings, database_embeddings)\n",
    "\n",
    "    # Create a mapping from index to filename\n",
    "    index_to_filename = {i: fname for i, fname in enumerate(database_filenames)}\n",
    "\n",
    "    # Create the mosaic image\n",
    "    print(\"Creating the mosaic image...\")\n",
    "    mosaic_image = Image.new('RGB', (new_width, new_height))\n",
    "    for idx, (x, y) in tqdm(enumerate(positions), total=len(positions)):\n",
    "        nearest_index = nearest_indices[idx]\n",
    "        nearest_filename = index_to_filename[nearest_index]\n",
    "        # Load the patch image\n",
    "        patch_image_path = os.path.join(patches_folder, nearest_filename)\n",
    "        patch_image = Image.open(patch_image_path).convert('RGB')\n",
    "        patch_image = patch_image.resize((8, 8))\n",
    "        # Paste the patch into the mosaic image\n",
    "        mosaic_image.paste(patch_image, (x, y))\n",
    "\n",
    "    # Save the mosaic image\n",
    "    mosaic_image.save(output_image_path)\n",
    "    print(f\"Mosaic image saved to {output_image_path}\")\n",
    "\n",
    "    # Clean up\n",
    "    del vgg_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    sr_image_path = 'photo_2024-09-30_16-47-00_sr.jpg'  # Replace with your SR image path\n",
    "    patches_folder = 'patches'  # Folder containing the patches\n",
    "    embeddings_csv = 'perceptual_embeddings.csv'  # CSV file with embeddings\n",
    "    output_image_path = 'mosaic_image.jpg'  # Output mosaic image path\n",
    "\n",
    "    # Create the mosaic image\n",
    "    create_mosaic_image(sr_image_path, patches_folder, embeddings_csv, output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77b948-5cb6-42e2-8f3a-f77f163edcc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrast",
   "language": "python",
   "name": "contrast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
